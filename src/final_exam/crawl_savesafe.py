import requests
from bs4 import BeautifulSoup
import csv
import time
from datetime import datetime

# åˆ†é¡èˆ‡å°æ‡‰ t_s_id
categories = {
    "ç±³æ²¹ç½é ­æ³¡éºµ": [
        ("ç±³/äº”ç©€/æ¿ƒæ¹¯", "42649"),
        ("æ²¹/èª¿å‘³æ–™", "42652"),
        ("æ³¡éºµ / éºµæ¢", "42660"),
        ("ç½é ­èª¿ç†åŒ…", "42644"),
    ],
    "é¤…ä¹¾é›¶é£Ÿé£²æ–™": [
        ("ä¼‘é–’é›¶å˜´", "39140"),
        ("ç¾å‘³é¤…ä¹¾", "39198"),
        ("ç³–æœ/å·§å…‹åŠ›", "62362"),
        ("é£²æ–™", "39153"),
    ],
    "å¥¶ç²‰é¤Šç”Ÿä¿å¥": [
        ("é¤Šç”Ÿä¿å¥/å¸¸å‚™å“", "43047"),
        ("å¥¶ç²‰/ç©€éº¥ç‰‡", "43046"),
        ("ç‰¹è‰²èŒ¶å“", "43053"),
        ("å’–å•¡/å¯å¯", "43049"),
    ],
    "æ²æµ´é–‹æ¶ä¿é¤Š": [
        ("æ²æµ´ä¹³é¦™çš‚", "42661"),
        ("ç¾é«®é€ å‹", "42640"),
        ("å£è…”æ¸…æ½”", "42638"),
        ("è‡‰éƒ¨æ¸…æ½”", "42659"),
        ("é–‹æ¶/èº«é«”ä¿é¤Š", "42643"),
    ],
    "é¤å»šè¡›æµ´å±…å®¶": [
        ("é‹å…·/é£²æ°´/å»šæˆ¿", "39213"),
        ("æƒé™¤ç”¨å…·/ç…§æ˜/äº”é‡‘", "44678"),
        ("å‚¢é£¾/æ”¶ç´/è¡›æµ´", "39202"),
        ("å¯µç‰©/åœ’è—", "39189"),
    ],
    "æ—¥ç”¨æ¸…æ½”ç”¨å“": [
        ("è¡›ç”Ÿç´™/æ¿•å·¾", "42657"),
        ("è¡£ç‰©æ¸…æ½”", "42637"),
        ("å±…å®¶æ¸…æ½”", "42642"),
        ("è¡›ç”Ÿæ£‰/è­·å¢Š", "54722"),
        ("æˆäºº/å¬°å…’ç´™å°¿è¤²", "54952"),
    ],
    "å®¶é›»/3Cé…ä»¶": [
        ("å»šæˆ¿å®¶é›»", "39220"),
        ("å­£ç¯€å®¶é›»", "39197"),
        ("ç”Ÿæ´»å®¶é›»", "47024"),
        ("3C/é›»è…¦å‘¨é‚Š/OA", "39172"),
    ],
    "æ–‡å…·ä¼‘é–’æœé£¾": [
        ("æ–‡å…·/è¾¦å…¬ç”¨å“", "57242"),
        ("æ±½æ©Ÿè»Šç™¾è²¨", "39233"),
        ("ä¼‘é–’/é‹å‹•", "39203"),
        ("æœé£¾/é…ä»¶", "67217"),
    ],
    "å…é‹/ä¸»é¡Œå°ˆå€": [
        ("å†·å‡å…é‹", "67217"),
        ("ç®±è³¼å…é‹", "67742"),
        ("ä¸»é¡Œå°ˆå€", "67745"),
    ],
}

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36"
}


def fetch_product_detail(product_url, product_name, max_retries=3):
    """çˆ¬å–å•†å“è©³ç´°é ï¼ŒåŠ å…¥é‡è©¦æ©Ÿåˆ¶å’Œè©³ç´° log"""
    for attempt in range(max_retries):
        try:
            print(
                f"  â†’ Fetching detail for: {product_name[:30]}... (attempt {attempt + 1}/{max_retries})"
            )
            r = requests.get(product_url, headers=headers, timeout=10)
            if r.status_code != 200:
                print(f"    âœ— Failed with status code: {r.status_code}")
                return ""
            soup = BeautifulSoup(r.text, "html.parser")

            # æ­£ç¢ºçš„é¸æ“‡å™¨ï¼šProductDescriptionListArea
            detail_section = soup.select_one("div.ProductDescriptionListArea")

            if not detail_section:
                # å‚™é¸æ–¹æ¡ˆ
                detail_section = soup.select_one("div.ProductDescriptionArea")

            if detail_section:
                # å–å¾—æ‰€æœ‰æ–‡å­—å…§å®¹
                detail_text = detail_section.get_text(separator="\n", strip=True)
                print(f"    âœ“ Successfully fetched detail ({len(detail_text)} chars)")
                return detail_text
            else:
                print(f"    âš  No detail section found")
                return ""
        except requests.exceptions.Timeout:
            print(f"    â± Timeout on attempt {attempt + 1}/{max_retries}")
            if attempt < max_retries - 1:
                time.sleep(2)
                continue
            print(f"    âœ— Failed after {max_retries} attempts")
            return ""
        except requests.exceptions.RequestException as e:
            print(f"    âœ— Error: {e}")
            return ""


def fetch_products(category_id, category_name, subcategory_name):
    base_url = "https://www.savesafe.com.tw/Products/ProductList"
    page = 1
    products = []

    while True:
        try:
            params = {"t_s_id": category_id, "Pg": page, "s": 6}
            r = requests.get(base_url, params=params, headers=headers, timeout=10)
            if r.status_code != 200:
                print(
                    f"Failed to fetch page {page} from {subcategory_name} (Category: {category_name}), status code: {r.status_code}"
                )
                break

            soup = BeautifulSoup(r.text, "html.parser")
            product_blocks = soup.select("div.col.mb-4.text-left.NewActivityItem")
            if not product_blocks:
                print(
                    f"No products found on page {page} for {subcategory_name} (Category: {category_name}), stopping."
                )
                break

            print(f"Processing page {page} with {len(product_blocks)} products...")

            for idx, block in enumerate(product_blocks, 1):
                sku = (
                    block.select_one("input#data_Prd_Sku")["value"]
                    if block.select_one("input#data_Prd_Sku")
                    else ""
                )
                attr_no = (
                    block.select_one("input#data_Prd_Attribute_Item_No")["value"]
                    if block.select_one("input#data_Prd_Attribute_Item_No")
                    else ""
                )
                prdatt_sid = (
                    block.select_one("input#PrdAtt_SID")["value"]
                    if block.select_one("input#PrdAtt_SID")
                    else ""
                )
                img_tag = block.select_one("img.card-img-top")
                img_url = img_tag["src"] if img_tag else ""
                link_tag = block.select_one('a[href^="ProductView"]')
                if link_tag:
                    href = link_tag["href"]
                    if not href.startswith("/"):
                        href = "/" + href
                    if href.startswith("/ProductView"):
                        href = "/Products" + href
                    link = "https://www.savesafe.com.tw" + href
                else:
                    link = ""
                name_tag = block.select_one("p.card-title.ItemName")
                name = name_tag.text.strip() if name_tag else ""
                description_tag = block.select_one("p.mb-2.ObjectName")
                description = description_tag.text.strip() if description_tag else ""
                price_tag = block.select_one("span.Price")
                price = price_tag.text.strip() if price_tag else ""

                print(f"[{idx}/{len(product_blocks)}] Processing: {name[:40]}...")
                description_detail = fetch_product_detail(link, name) if link else ""

                products.append(
                    {
                        "sku": sku,
                        "attribute_no": attr_no,
                        "prdatt_sid": prdatt_sid,
                        "name": name,
                        "description": description,
                        "description_detail": description_detail,
                        "price": price,
                        "image_url": img_url,
                        "product_link": link,
                        "category": category_name,
                        "subcategory": subcategory_name,
                    }
                )

            print(
                f"âœ“ Fetched {len(product_blocks)} products from page {page} of {subcategory_name} (Category: {category_name})"
            )

            next_page_link = soup.select_one(f'a[href*="Pg={page+1}"]')
            if not next_page_link:
                print(
                    f"No next page for {subcategory_name} (Category: {category_name}), done."
                )
                break

            page += 1
            time.sleep(1)

        except requests.exceptions.Timeout:
            print(f"Timeout fetching page {page}, retrying...")
            time.sleep(3)
            continue
        except Exception as e:
            print(f"Error on page {page}: {e}")
            break

    return products


def save_to_csv_append(products, filename, write_header=False):
    """è¿½åŠ æ¨¡å¼å¯«å…¥ CSV"""
    if not products:
        print("No data to save.")
        return
    keys = products[0].keys()
    mode = "a" if not write_header else "w"
    with open(filename, mode, newline="", encoding="utf-8-sig") as f:
        dict_writer = csv.DictWriter(f, keys)
        if write_header:
            dict_writer.writeheader()
        dict_writer.writerows(products)
    print(f"ğŸ’¾ Saved {len(products)} products to {filename}\n")


if __name__ == "__main__":
    # ç”Ÿæˆå¸¶æ™‚é–“æˆ³è¨˜çš„æª”æ¡ˆåç¨±
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"./output/savesafe_products_{timestamp}.csv"

    print(f"\n{'='*80}")
    print(f"SaveSafe Product Crawler")
    print(f"Output file: {filename}")
    print(f"{'='*80}\n")

    first_write = True
    total_products = 0

    for category_name, subcats in categories.items():
        for subcat_name, t_s_id in subcats:
            print(f"\n{'='*80}")
            print(f"Start crawling {subcat_name} under {category_name}")
            print(f"{'='*80}")
            prods = fetch_products(t_s_id, category_name, subcat_name)

            # æ¯å€‹å­åˆ†é¡çˆ¬å®Œå°±å¯«å…¥
            save_to_csv_append(prods, filename, write_header=first_write)
            first_write = False
            total_products += len(prods)

    print(f"\n{'='*80}")
    print(f"âœ“ Crawling completed! Total products: {total_products}")
    print(f"âœ“ Data saved to: {filename}")
    print(f"{'='*80}")
